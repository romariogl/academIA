import sys
import os

# Adiciona o diret√≥rio raiz do projeto ao PATH
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from rag_backend.models.chroma_vector_store import ChromaVectorStore
from bs4 import BeautifulSoup
import requests
from langchain_community.document_loaders import WebBaseLoader
from sentence_transformers import SentenceTransformer 

# Configura√ß√µes do ChromaDB
vector_store = ChromaVectorStore()

# Fun√ß√£o para indexar um chunk no ChromaDB
def index_chunk(index_name, chunk, article_name, article_id, url):
    body = {
        "article_name": article_name,
        "content": chunk,
        "article_fulldoc_url": url,
        "article_id": article_id
    }
    vector_store.index(index_name=index_name, id=article_id, body=body)

# Fun√ß√£o para extrair o t√≠tulo do artigo da tag meta
def extract_article_title_text_and_fulldocurl(url):
    response = requests.get(url)
    if response.status_code != 200:
        raise ValueError(f"Erro ao acessar a URL: {url}")    
    soup = BeautifulSoup(response.text, "html.parser")
    
    meta_tag = soup.find("meta", attrs={"name": "title"})
    if not meta_tag or not meta_tag.get("content"):
        raise ValueError("N√£o foi poss√≠vel encontrar a meta tag com o t√≠tulo do artigo.")    
    title = meta_tag.get("content")
    print(f"T√≠tulo: {title}")

    meta_tag = soup.find("meta", attrs={"name": "description"})
    if not meta_tag or not meta_tag.get("content"):
        raise ValueError("N√£o foi poss√≠vel encontrar a meta tag com a descri√ß√£o do artigo.")    
    description = meta_tag.get("content")
    print(f"Descri√ß√£o: {description}")

    button = soup.find("a", id="item-acessar")
    if button and button.has_attr("href"):
        href = button["href"]
        print(f"Link 'Acessar' encontrado: {href}")
    else:
        print("Bot√£o/link 'Acessar' n√£o encontrado.")
        href = url  # Fallback para a URL original

    return title, description, href

# Carrega os dados do website e processa os chunks
def fetch_and_process_website_summary(url):
    # 1. Extrair o t√≠tulo do artigo
    article_title, description, fulldoc_url = extract_article_title_text_and_fulldocurl(url)
    print(f"T√≠tulo do artigo extra√≠do: {article_title}")

    # 2. Dividir o conte√∫do em chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    document = Document(page_content=description, metadata={"title": article_title, "article_fulldoc_url": fulldoc_url})
    chunks = text_splitter.split_documents([document])
    print(f"Dividiu os documentos em {len(chunks)} chunks.")

    # 3. Ingerir os chunks no ChromaDB
    for i, chunk in enumerate(chunks):
        chunk_id = f"{article_title}_summary_chunk_{i}"
        index_chunk(index_name="summary_index", chunk=chunk.page_content, article_id=chunk_id, article_name=article_title, url=fulldoc_url)
        print(f"Indexou chunk de resumo com ID: {chunk_id}")

    # 4. Processar documento completo
    fetch_and_process_website_full(url=fulldoc_url, article_title=article_title)    

# Carrega os dados do website e processa os chunks
def fetch_and_process_website_full(url, article_title):
    try:
        loader = WebBaseLoader(url)
        documents = loader.load()
        
        # Dividir o conte√∫do em chunks
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
        chunks = text_splitter.split_documents(documents)
        print(f"Dividiu os documentos completos em {len(chunks)} chunks.")

        # Ingerir os chunks no ChromaDB
        for i, chunk in enumerate(chunks):
            chunk_id = f"{article_title}_full_chunk_{i}"
            index_chunk(index_name="full_document_index", chunk=chunk.page_content, article_id=chunk_id, article_name=article_title, url=url)
            print(f"Indexou chunk completo com ID: {chunk_id}")

        print(f"Todos os chunks foram ingeridos no ChromaDB.")
        
    except Exception as e:
        print(f"Erro ao processar documento completo: {e}")

def extract_website_urls():
    capes_ia_search_url = "https://www.periodicos.capes.gov.br/index.php/acervo/buscador.html?q=intelig%C3%AAncia+artificial&source=&publishyear_min%5B%5D=1943&publishyear_max%5B%5D=2025&page="
    capes_ia_articles_urls = []
    
    # Limitar a 2 p√°ginas para teste (pode ser aumentado)
    for i in range(2):
        try:
            response = requests.get(capes_ia_search_url + str(i))
            if response.status_code != 200:
                print(f"Erro ao acessar p√°gina {i}")
                continue
                
            soup = BeautifulSoup(response.text, "html.parser")
            links = soup.find_all('a', class_='titulo-busca')

            for link in links:
                url = "https://www.periodicos.capes.gov.br" + link.get('href')
                capes_ia_articles_urls.append(url)
                
        except Exception as e:
            print(f"Erro ao processar p√°gina {i}: {e}")
    
    return capes_ia_articles_urls

# Fun√ß√£o para carregar dados de exemplo (para teste sem internet)
def load_sample_data():
    """Carrega dados de exemplo para teste"""
    sample_docs = [
        {
            "title": "Intelig√™ncia Artificial na Educa√ß√£o",
            "content": "A intelig√™ncia artificial est√° revolucionando a educa√ß√£o atrav√©s de sistemas adaptativos e personaliza√ß√£o do aprendizado. Os algoritmos de machine learning permitem identificar padr√µes de aprendizado individuais e adaptar o conte√∫do educacional de acordo com as necessidades espec√≠ficas de cada aluno.",
            "url": "https://exemplo.com/ia-educacao"
        },
        {
            "title": "Machine Learning em Medicina",
            "content": "O machine learning tem aplica√ß√µes importantes na medicina, incluindo diagn√≥stico precoce e an√°lise de imagens m√©dicas. Algoritmos de deep learning s√£o capazes de detectar anomalias em radiografias e tomografias com precis√£o superior √† humana em muitos casos.",
            "url": "https://exemplo.com/ml-medicina"
        },
        {
            "title": "Chatbots e Processamento de Linguagem Natural",
            "content": "Chatbots modernos utilizam t√©cnicas avan√ßadas de processamento de linguagem natural para melhorar a intera√ß√£o com usu√°rios. Modelos como GPT e BERT revolucionaram a capacidade de compreens√£o e gera√ß√£o de texto natural.",
            "url": "https://exemplo.com/chatbots-nlp"
        },
        {
            "title": "√âtica em Intelig√™ncia Artificial",
            "content": "A √©tica em intelig√™ncia artificial √© fundamental para garantir que os sistemas de IA sejam desenvolvidos e utilizados de forma respons√°vel. Quest√µes como vi√©s algor√≠tmico, privacidade e transpar√™ncia s√£o cruciais para o futuro da tecnologia.",
            "url": "https://exemplo.com/etica-ia"
        },
        {
            "title": "Redes Neurais e Deep Learning",
            "content": "Redes neurais profundas s√£o a base do deep learning moderno. Essas arquiteturas complexas permitem que m√°quinas aprendam representa√ß√µes hier√°rquicas de dados, desde caracter√≠sticas simples at√© conceitos abstratos complexos.",
            "url": "https://exemplo.com/redes-neurais"
        }
    ]
    
    for i, doc in enumerate(sample_docs):
        # Indexar resumo
        chunk_id = f"{doc['title']}_sample_summary_{i}"
        index_chunk(
            index_name="summary_index", 
            chunk=doc['content'], 
            article_id=chunk_id, 
            article_name=doc['title'], 
            url=doc['url']
        )
        
        # Indexar documento completo (simular documento mais longo)
        chunk_id = f"{doc['title']}_sample_full_{i}"
        index_chunk(
            index_name="full_document_index", 
            chunk=doc['content'] * 3,  # Simular documento mais longo
            article_id=chunk_id, 
            article_name=doc['title'], 
            url=doc['url']
        )
    
    print("Dados de exemplo carregados com sucesso no ChromaDB!")

# Executa o processamento e ingest√£o
if __name__ == "__main__":
    print("üöÄ Iniciando ingest√£o de dados no ChromaDB...")
    
    # Op√ß√£o 1: Carregar dados de exemplo (para teste r√°pido)
    load_sample_data()
    
    # Op√ß√£o 2: Extrair dados reais do CAPES (descomente se quiser usar)
    # website_urls = extract_website_urls()
    # print(f"Encontradas {len(website_urls)} URLs")
    # 
    # for url in website_urls:
    #     try:
    #         fetch_and_process_website_summary(url)
    #     except Exception as e:
    #         print(f"Erro ao processar {url}: {e}")
    
    print("‚úÖ Ingest√£o conclu√≠da no ChromaDB!")